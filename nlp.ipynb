{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9633c5e-798d-4ce0-8f6d-a7cd44b6f4f9",
   "metadata": {},
   "source": [
    "## **Dataset Used:** I have used  imdb-sentiment dataset.which have 1 lakh  movie reviews and their labels Due to in-convinience i have redeced them to 10K\n",
    "\n",
    "## **Link :** https://www.kaggle.com/datasets/jcblaise/imdb-sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0ff8d-dd84-4b42-a6b6-e4eca97c1190",
   "metadata": {},
   "source": [
    "## **Description :**\n",
    "\n",
    "1. Natural Language Processing (NLP) is a field of artificial intelligence and computational linguistics that deals with the interaction between computers and human language.\n",
    "\n",
    "2.  NLP techniques are used to analyze and process human language, including text, speech, and handwriting, in order to extract meaning and understanding. \n",
    "\n",
    "3. This allows computers to understand and respond to natural language input, as well as generate human-like output. \n",
    "\n",
    "4. NLP applications include sentiment analysis, text classification, machine translation, and text-to-speech synthesis. \n",
    "\n",
    "5. The process of NLP typically involves the following steps:\n",
    "\n",
    "    * Text Preprocessing: The raw text data is cleaned, normalized, and prepared for analysis. This includes tasks such as removing stop words, stemming, and removing punctuation and special characters.\n",
    "\n",
    "    * Tokenization: The preprocessed text is then divided into individual units, known as tokens, which can be words, phrases, or symbols.\n",
    "\n",
    "    * Part-of-Speech Tagging: Tokens are then tagged with their respective parts of speech, such as nouns, verbs, and adjectives.\n",
    "\n",
    "    * Named Entity Recognition: Named entities, such as people, organizations, and locations, are extracted and identified.\n",
    "\n",
    "    * Parsing and Dependency Analysis: The relationships between words in the text are analyzed, including their dependencies and syntactic structures.\n",
    "\n",
    "    * Semantic Analysis: The meaning of the text is extracted and analyzed, including sentiment analysis, summarization, and topic modeling.\n",
    "\n",
    "    * Generation of Output: The processed and analyzed text is used to generate output, such as a response to a question, a summary, or a machine-generated text.\n",
    "\n",
    "    * Evaluation: The performance of the NLP model is evaluated using metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "6. The process of NLP can be complex, and different techniques may be used depending on the specific task and the nature of the text data. Additionally, the quality and accuracy of the NLP results can be influenced by factors such as the size of the training data and the complexity of the language being analyzed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980963fe-d97c-4b03-afa8-cd1001982084",
   "metadata": {},
   "source": [
    "## **Source Code :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76c91307-b83b-4951-9934-63adab1e68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25af38b-c9c8-410f-9b43-0ee0ff17f308",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6cb380e-1b8d-465e-8264-57cf170f4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"archive//train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e58124-bdb0-4734-8067-d7da1de1b6d7",
   "metadata": {},
   "source": [
    "### Visualising dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdf97b73-7d26-483d-a9d6-f15292916c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>My comments may be a bit of a spoiler, for wha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>The \"saucy\" misadventures of four au pairs who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>Oh, those Italians! Assuming that movies about...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Eight academy nominations? It's beyond belief....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>Not that I dislike childrens movies, but this ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0      For a movie that gets no respect there sure ar...          0\n",
       "1      Bizarre horror movie filled with famous faces ...          0\n",
       "2      A solid, if unremarkable film. Matthau, as Ein...          0\n",
       "3      It's a strange feeling to sit alone in a theat...          0\n",
       "4      You probably all already know this by now, but...          0\n",
       "...                                                  ...        ...\n",
       "24995  My comments may be a bit of a spoiler, for wha...          1\n",
       "24996  The \"saucy\" misadventures of four au pairs who...          1\n",
       "24997  Oh, those Italians! Assuming that movies about...          1\n",
       "24998  Eight academy nominations? It's beyond belief....          1\n",
       "24999  Not that I dislike childrens movies, but this ...          1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8873338-5a73-4d97-a8bf-f1890bc9ff2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d406a65c-9f0e-4f93-a978-5078be464d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c65c6-0d21-4247-b40d-b509e217749c",
   "metadata": {},
   "source": [
    "### Imporint nltk(natural language processing toolkit) and re(regular expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "757e3e9d-216f-4d28-9225-c148831e631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6aa0cad-dbf3-4ce0-b49e-e3bec480f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\99406\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5976921-a9c4-4b0e-8532-87acced4344f",
   "metadata": {},
   "source": [
    "### Importing stop words and port stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b121b06-7b8b-4387-b9d4-5932db6fbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb16aa76-1507-4ba9-97d8-3afade17a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "all_stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb64f3-9570-46c4-a150-12b4fd433a47",
   "metadata": {},
   "source": [
    "### Removing not and didn't from stop words because\n",
    "\n",
    "ex :\n",
    "\n",
    "I did not like it(negative) -> (removing not makes it ) (I did like it)(positive)\n",
    "      \n",
    "      I did't like it(negative) -> (removing not makes it ) (I like it)(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0de95990-f4b3-46da-a461-60f57fee97f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_words.remove(\"not\")\n",
    "all_stop_words.remove(\"didn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d9430-495b-4b89-ba65-fa0f8cdf6ccf",
   "metadata": {},
   "source": [
    "### Stemming every word in all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8933b78b-65bf-42ed-b9d1-353a236f7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,25000):\n",
    "     review = re.sub(\"[^a-zA-Z']\",' ',data[\"text\"][i])\n",
    "     review = review.lower()\n",
    "     review = review.split()\n",
    "     review = [ps.stem(word) for word in review if  word not in all_stop_words]\n",
    "     review = \" \".join(review)\n",
    "     corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60f518ac-920c-43d1-a80a-beb138c7e37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35675"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705deec-2079-4290-bcb2-8137b8fba516",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "011afd44-87f3-4ad1-9cba-977f2289552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "696181e1-d885-49cd-80e8-dcace0adc773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10675, 1500) (25000,)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "Y = data[\"sentiment\"]\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7508a0bc-2992-4cee-b47d-3b6e5f14c062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287e57a-0ffe-4cfd-80c8-ea1da7fd922b",
   "metadata": {},
   "source": [
    "### Splitting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f20d8645-70c2-40c0-99df-f3dfe9c2a9bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10675, 25000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m X_train,X_test,Y_train,Y_test \u001b[38;5;241m=\u001b[39m train_test_split(X,Y,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.30\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,stratify\u001b[38;5;241m=\u001b[39mY)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2657\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2662\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10675, 25000]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.30,random_state=6,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c40d69-91b9-4040-961c-59ffef62a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a12fb-5d68-41dd-9886-6b08196dc628",
   "metadata": {},
   "source": [
    "### Implementing GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d829c-e833-422f-819f-a83963f098e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,Y_train)\n",
    "print(\"Score = \",nb.score(X_test,Y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ca629-99a7-492b-b483-5b1656875ceb",
   "metadata": {},
   "source": [
    "### Predicting results and printing confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700682a-0224-4673-8e2c-fb1cf102a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = nb.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "matrix = confusion_matrix(Y_test,Y_pred)\n",
    "report = classification_report(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6def1-1f89-48a9-89bc-23958b2951ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8ada0-a1fd-4b1f-a0a9-ecf0584370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e012a-10a4-4dca-918a-0269387e2f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07b052-2aa3-4445-968d-74d5fdab86b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a052af-1dc3-4b0c-8d82-4bd5c4d6fddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8506098-36c6-433c-bc96-6ad32d3bd0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96464a4a-8790-4a0d-b7a7-1adb328746dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d4684-9d07-47cf-a954-ce8b9aeecfde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
